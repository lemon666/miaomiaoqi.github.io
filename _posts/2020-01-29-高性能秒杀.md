---
layout: post
title: 高性能秒杀
categories: [project]
description: 
keywords: 
---

* content
{:toc}


## 云端部署

### 数据库部署

备份

```
mysqldump -uroot -p --databases seckill > ~/Downloads/seckill.sql
```

导入

```
mysql -uroot -p < /usr/local/seckill.sql
```

### 打包上传

本地打包

```bash
mvn clean package
```

上传到服务器

```bash
scp imooc-seckill-1.0-SNAPSHOT.jar root@122.51.237.121:/usr/local
```

登录服务器创建目录

```bash
mkdir -p /var/www/seckill
```

移动 jar 包

```bash
cd /var/www/seckill
mv /usr/local/imooc-seckill-1.0-SNAPSHOT.jar seckill.jar
```

授权

```bash
chmod -R 777 *
```

启动程序

```bash
java -jar seckill.jar
```



### 编写 deploy

```bash
cd /var/www/seckill
```

编写外挂配置

```bash
vim application.properties

server.port=8088
```

重新启动程序, 指定外挂配置

```bash
java -jar seckill.jar --spring.config.addition-locaotion=/var/www/seckill/application.properties
```

编写 deploy 脚本

```bash
vim deploy.sh

nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar seckill.jar --spring.config.addition-locaotion=/var/www/seckill/application.properties
```



## jemeter 压测

### 线程组

### http 请求

### 查看结果树

### 聚合报告

![https://miaomiaoqi.github.io/images/project/seckill/seckill_4.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_4.png)

90% Line: 90% 的请求落在这个毫秒内, 反之就是有 10% 的请求大于这个毫秒

95% Line: 95% 的请求落在这个毫秒内

99% Line: 95% 的请求落在这个毫秒内

### 查看服务器状态

使用 top 命令可以查看 cpu 的使用状态

```bash
top -H
```

![https://miaomiaoqi.github.io/images/project/seckill/seckill_5.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_5.png)

**CPU usage: 7.12% user, 6.3% sys:** 分表代表用户态和系统态的 CPU 使用率, 加在一起不超过 100%

**Load Avg: 2.88, 3.10, 3.08:** 1 分钟, 5 分钟, 15 分钟 CPU 的使用负载, 不超过服务器 CPU 个数最好, load 越高说明 CPU 耗的越多

## Tomcat 调优

我们使用 5000 个线程循环 20 次对接口 /item/get?id=8 进行压测, 最终会报错显示 tomcat 拒绝连接, 初步判断是 tomcat 的线程数不够导致的该问题

### 查看线程数

通过使用 `pstree -p pid | wc -l` 查看 java 服务的线程数发现只有 39 个线程(29 个其他线程), 导致客户端请求不能建立新的连接发生报错 mac 使用`ps -M pid | wc -l`

### **查看内嵌 tomcat 配置**

spring-boot-autoconfigure 包下的 spring-configuration-metadata.json 文件查看各个节点的配置可以看到内嵌 tomcat 的默认配置

**默认内嵌 tomcat 配置**

server.tomact.accept-count: 等待队列长度, 默认 100

server.tomcat.max-connections: 最大可被连接数, 默认 10000

server.tomcat.max-threads: 最大工作线程数, 默认 200

server.tomcat.min-spare-threads: 最小工作线程数, 默认 10

默认配置下, 连接超过 10000 后出现拒绝连接情况

默认配置下, 触发的请求超过 200+100 后拒绝处理

**理解 maxConnections, maxThreads, acceptCount 关系**

我们可以把tomcat比做一个火锅店，流程是取号、入座、叫服务员，可以做一下三个形象的类比：

1. acceptCount 最大等待数

    可以类比为火锅店的排号处能够容纳排号的最大数量；排号的数量不是无限制的，火锅店的排号到了一定数据量之后，服务往往会说：已经客满。

2. maxConnections 最大连接数

    可以类比为火锅店的大堂的餐桌数量，也就是可以就餐的桌数。如果所有的桌子都已经坐满，则表示餐厅已满，已经达到了服务的数量上线，不能再有顾客进入餐厅了。

3. maxThreads 最大线程数

    可以类比为厨师的个数。每一个厨师，在同一时刻，只能给一张餐桌炒菜，就像极了JVM中的一条线程

**整个就餐的流程, 大致如下**

1. 取号: 如果maxConnections连接数没有满，就不需要取号，因为还有空余的餐桌，直接被大堂服务员领上餐桌，点菜就餐即可。如果 maxConnections 连接数满了，但是取号人数没有达到 acceptCount，则取号成功。如果取号人数已达到acceptCount，则拿号失败，会得到Tomcat的Connection refused connect 的回复信息。
2. 上桌: 如果有餐桌空出来了，表示maxConnections连接数没有满，排队的人，可以进入大堂上桌就餐。
3. 就餐: 就餐需要厨师炒菜。厨师的数量，比顾客的数量，肯定会少一些。一个厨师一定需要给多张餐桌炒菜，如果就餐的人越多，厨师也会忙不过来。这时候就可以增加厨师，一增加到上限maxThreads的值，如果还是不够，只能是拖慢每一张餐桌的上菜速度，这种情况，就是大家常见的“上一道菜吃光了，下一道菜还没有上”尴尬场景。

### 修改配置

```properties
server.port=8088
# 等待队列长度, 默认100
server.tomact.accept-count=1000
# 最大工作线程数, 默认200, 4核8g内存, 线程数经验值800
# 操作系统做线程之间的切换调度是有系统开销的, 所以不是越多越好
server.tomcat.max-threads=800
# 最小工作空闲线程数, 默认10, 适当增大一些, 以便应对突然增长的访问量
server.tomcat.min-spare-threads=100
```

**修改参数后启动程序使用 ps -M pid|wc -l 查看发现线程数增多到 129 个, 再次使用 2000 个线程循环 50 次进行压测, 可以看到线程数增加到 832 个, 虽然最终也会报错, 但是不会像默认配置一样很快就发生报错, 说明提高 tomcat 线程数可以增强系统的访问能力**



### 定制化内嵌 Tomcat 开发

**如果我们 keepalive 一直保持连接, 但是网页一直没有操作, 就会占用线程浪费资源**

keepAliveTimeOut: 多少毫秒后不响应的断开 keepalive

maxKeepAliveRequests: 多少次请求后 keepalive 断开失效

使用 WebServerFactoryCustomizer\<ConfigurableServletWebServerFactory\> 定制化内嵌 tomcat 配置

```java
/**
 * 当 Spring 容器内没有 TomcatEmbeddedServletContainerFactory 这个 bean 时，会吧此 bean 加载进 spring 容器中
 *
 * @author miaoqi
 * @date 2020-01-29
 *
 * @return
 */
@Component
public class WebServerConfiguration implements WebServerFactoryCustomizer<ConfigurableWebServerFactory> {

    @Override
    public void customize(ConfigurableWebServerFactory factory) {
        // 使用对应工厂类提供给我们的接口定制化我们的 tomcat connector
        ((TomcatServletWebServerFactory) factory).addConnectorCustomizers(new TomcatConnectorCustomizer() {
            @Override
            public void customize(Connector connector) {
                Http11NioProtocol protocol = (Http11NioProtocol) connector.getProtocolHandler();
                // 定制化keepalivetimeout,设置30秒内没有请求则服务端自动断开keepalive链接
                protocol.setKeepAliveTimeout(30000);
                // 当客户端发送超过10000个请求则自动断开keepalive链接
                protocol.setMaxKeepAliveRequests(10000);
            }
        });
    }

}
```



### 单 Web 容器上线

**线程数量:** 4 核 cpu 8G 内存单进程调度线程数 800-1000以上后即花费巨大的时间在 cpu 调度上

**等待队列长度:** 队列做缓冲池用, 但也不能无限长, 消耗内存, 出队入队也消耗 cpu



### MySql 数据库 QPS 容量问题

**主键查询:** 千万级别数据 = 1-10 毫秒

**唯一索引查询:** 千万级别数据 = 10-100 毫秒

**非唯一索引查询:** 千万级别数据 = 100-1000 毫秒

**无索引:** 百万条数据 = 1000 毫秒+

### MySql 数据库 TPS 容量问题

更新删除操作: 同查询

插入操作: 1W ~ 10W tps(依赖配置优化)





## 分布式扩展

![https://miaomiaoqi.github.io/images/project/seckill/seckill_6.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_6.png)

我使用 600 个线程, 循环 50 次, 10 秒内启动完成, 理论上每秒应该处理 3000 个请求, 而实际上 tps 只有 2500 左右, **证明单机达到了上限, 多余的请求进行排队处理**

**单机容量问题, 水平扩展**

表象: 单机 cpu 使用率增高, memory 占用增加, 网络带宽使用增加

cpu us: 用户空间的 cpu 使用情况(用户层代码)

cpu sy: 内核空间的 cpu 使用情况(系统调用)

load average: 1,5,15 分钟 load 平均值, 根据核数系数, 0 代表通常, 1(核数) 代表打满, 1+代表等待阻塞

memory: free 空闲内存, used 使用内存

### nginx 反向代理负载均衡

到 [http://openresty.org/en](http://openresty.org/en) 下载 openresty(基于 nginx 做了二次封装许多 lua 脚本, 方便使用)

使用 `sbin/nginx -c /usr/local/etc/openresty/nginx.conf` 命令启动 openresty

location 节点 path: 指定 url 映射 key

location 节点内容: root 指定 location path 后对应的根路径, index 指定默认的访问页

sbin/nginx -c conf/nginx.conf 启动

修改配置后直接 sbin/nginx -s reload 无缝重启

#### nginx 部署前端资源

1. 拷贝前端资源到 openresty 的 html/resources 目录下

2. 修改 nginx.conf 配置文件, 将所有 resources/ 请求都映射到 html/resources 目录下

    ```yaml
    server {
      listen       80;
      server_name  localhost;
    
      #charset koi8-r;
    
      #access_log  logs/host.access.log  main;
    
      #location / {
      	#root   html;
      	#index  index.html index.htm;
      #}
      #
      location /resources/ {
      	alias /usr/local/Cellar/openresty/1.15.8.2/nginx/html/resources/;
      	index index.html index.htm;
      }
    
      #error_page  404              /404.html;
    
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      	location = /50x.html {
      	root   html;
      }
    
      # proxy the PHP scripts to Apache listening on 127.0.0.1:80
      #
      #location ~ \.php$ {
      #    proxy_pass   http://127.0.0.1;
      #}
    
      # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
      #
      #location ~ \.php$ {
      #    root           html;
      #    fastcgi_pass   127.0.0.1:9000;
      #    fastcgi_index  index.php;
      #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
      #    include        fastcgi_params;
      #}
    
      # deny access to .htaccess files, if Apache's document root
      # concurs with nginx's one
      #
      #location ~ /\.ht {
      #    deny  all;
      #}
    }
    ```

#### nginx 反向代理服务端

1. 设置 upstream server 指明后端服务地址

    ```yaml
    upstream backend_server {
    	server 127.0.0.1:8088 weight=1;
    	server 127.0.0.1:8089 weight=1;
    	# 开启 nginx 与 server 的 keepalive, 设置为 30 秒
    	keepalive 30;
    }
    ```

2. 设置动态请求 location 为 proxy pass 路径

    ```yaml
    location / {
    	# 将除去 /resource/ 的请求都当做动态请求, 进行反向代理, 轮询请求后端服务
      proxy_pass http://backend_server;
      # 经过代理 nginx 就成为了 client 端, 需要将原始 Host 传到服务端
      proxy_set_header Host $http_host:$proxy_port;
      # 传入真正的访问 ip
      proxy_set_header X-Real-IP $remote_addr;
      # nginx 作为代理服务器转发了请求
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      
      # 默认使用 http1.0 与 Connection=closed, 即用完就断开连接, 不支持 keepalive
      # 需要手动指定 http1.1 与 Connection="", 即代表支持 keepalive
      proxy_http_version 1.1;
      proxy_set_header Connection "";
    }
    ```

3. 开启 tomcat access log 验证, 修改 springboot 配置文件, 线上建议开启

    ```
    server.tomcat.accesslog.enabled=true
    server.tomcat.accesslog.directory=/Users/miaoqi/Documents/seckill/seckill8088/tomcat
    server.tomcat.accesslog.pattern=%h %l %u %t "%r" %s %b %D
    ```

    %h: 远端 host 即 ip 地址

    %l: 

    %u: 远端 user

    %t: 处理时长

    %r: http 请求的第一行

    %s: 返回状态码

    %b: 请求 response 的大小

    %D: 处理请求的时长

#### nginx 高性能的原因

1. epoll 多路复用

    java bio 模型, 阻塞进程式

    linux select 模型, 变更触发轮询查找, 有 1024 数量上限

    epoll 模型, 变更触发回调直接读取, 理论上无上限

2. master worker 进程模型

    ![https://miaomiaoqi.github.io/images/project/seckill/seckill_7.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_7.png)

    nginx 启动必定会启动一个 master 进程, 随后会根据 nginx.conf 中 worker_processes 的配置数量 fork 出相应数量的 worker 进程

    master 进程监听 80 端口, 当有一个连接请求过来时, 会让 worker 进程在内存中抢锁的方式来决定哪个 worker 进程接管这个 socket, worker 进程执行 accept 操作建立 socket 并且设置回调, 后续所有的操作都在该 worker 中完成

    `nginx -s reload` 操作不会重启 master 进程且已经连接的 socket 不受影响, master 会接管 worker 的 socket, 并且重新启动一个新的 worker 加载配置文件, 再将 socket 交给 worker 进程, 整个过程是在内存中完成的, 效率非常高

3. 协程机制

    依附于线程的内存模型, 切换开销小

    遇阻塞及归还执行权, 代码同步

    无需加锁



### 使用 redis 实现分布式会话存储

token





## 查询优化多级缓存

### 多级缓存的定义

用快速存取设备, 用内存

将缓存推到离用户最近的地方

脏缓存清理

### 掌握 redis 缓存(一级)

单机版

sentinal 哨兵模式

集群 cluster 模式

### 本地热点缓存(二级)

相比于 redis 减少了网络传输的开销, 同时降低了 redis 的压力

本地缓存有如下特点

1. 热点数据(每秒访问上千, 上万的数据)

2. 脏读非常不敏感(每台应用服务器都保存着一份数据, 很可能存在脏读的情况)

3. 内存可控

本地缓存数据比 redis 中的数据的生命周期更短, 是保存在 JVM 中的, 是很宝贵的空间, 在分布式部署中是很难全部清理掉的, 可以用 mq 做广播通知每台服务进行清理

本地缓存本质上是利用一个 Map 存储对应的 key/value 数据, 但是要考虑并发问题, 性能问题, 缓存过期问题, 自己实现起来非常麻烦, 可以使用 Google 提供的 Guava cache 解决

1. 可控制的大小和超时时间
2. 可配置的 lru 策略
3. 线程安全

### 掌握热点 nginx lua 缓存

#### nginx proxy cache 缓存, 修改 nginx 配置

1. nginx 反向代理前置
2. 依靠文件系统存索引级文件
3. 依靠内存缓存文件地址

```
# 申明一个缓存节点内容
# level 可以作二级目录
# keys_zone: 缓存的名称, 可以任意取
# inactive: 存取 7 天
# max_size: 文件系统最多存放 10g 内容, 达到后采取 lru 算法
proxy_cache_path /usr/local/etc/openresty/tmp_cache levels=1:2 keys_zone=tmp_cache:100m inactive=7d max_size=10g;
```

这种缓存方式是读取本地文件系统, 并不是读取内存数据, 效率反而变低, nginx 还提供了其他的解决方案

#### nginx lua

**nginx 协程**

1. nginx 的每一个 Worker 进程都是在 epoll 或 kqueue 这种事件模型之上, 封装成协程
2. 每一个请求都有一个协程进行处理
3. 即时 ngx_lua 需要运行 lua, 相对 c 有一定的开销, 但依旧能保证高并发能力

**nginx 协程机制**

1. nginx 每个工作进程创建一个 lua 虚拟机
2. 工作进程内的所有协程共享同一个 vm
3. 每个外部请求都由一个 lua 协程处理, 之间数据隔离
4. lua 代码调用 io 等异步接口时, 协程被挂起, 上下文数据
5. 自动保存, 不阻塞工作进程
6. io 异步操作完成后还原协程上下文, 代码继续执行

**nginx 处理阶段**

1. NGX_HTTP_POST_READ_PHASE = 0, // 读取请求头
2. NGX_HTTP_SERVER_REWRITE_PHASE, // 执行 rewirte -> rewrite_handler
3. NGX_HTTP_FIND_CONFIG_PHASE, // 根据 uri 替换 location
4. NGX_HTTP_REWRITE_PHASE, // 根据替换结果继续执行 rewrite -> rewrite_handler
5. NGX_HTTP_POST_REWRITE_PHASE, // 执行 rewrite 后处理
6. NGX_HTTP_PREACCESS_PHASE, // 认证预处理, 请求限制, 连接下肢 -> limit_conn_handler, limit_req_handler
7. NGX_HTTP_ACCESS_PHASE, // 认证处理 -> auth_basic_handler, access_handler
8. NGX_HTTP_POST_ACCESS_PHASE, // 认证后处理, 认证不通过, 丢包
9. NGX_HTTP_TRY_FILES_PHASE, // 尝试 try 标签
10. NGX_HTTP_CONTENT_PHASE, // 内容处理 -> static_handler
11. NGX_HTTP_LOG_PHASE // 日志处理 -> log_handler

## 静态资源 CDN

所有的 ajax 请求都是依附于静态资源 H5 的, 我们访问页面也是先访问 H5 资源, 如果 H5 没有做优化, 依旧不能提高我们的 TPS

假如我们通过 nginx 反向代理, 缓存优化, 通过直接访问后台接口 tps 可以达到 3000, 但是我们通过访问 H5, H5 在访问后台接口时, H5 的 tps 只能到 2000, 那么我们的瓶颈就是在访问 H5 上了, 我们可以使用 CDN 提高对静态资源的访问速度

![https://miaomiaoqi.github.io/images/project/seckill/seckill_8.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_8.png)

### DNS 用 CNAME 解析到源站

通过云服务商进行配置

我们访问 miaoshaserver.chuangzhijian.com 时, DNS 解析出来这是 CNAME 地址, 就会将这条请求发送到 CNAME 地址上, CNAME 对应的服务器会根据请求的 ip 解析出来一个就近的 CDN 节点返回给用户去访问, CDN 节点会判断有没有用户请求的资源,如果有的话就返回, 没有的话就会到源站(nginx)中查找并在自己服务器上保存一份

前端服务和后端应用服务分别使用两套 nginx, 两套域名, 前端的域名使用 CDN 加速提高静态数据的返回速度, 后端不需要

### 回源缓存设置(应用服务)

**cache control 响应头(response header)**

1. private: 客户端可以缓存, 代理服务器不可以缓存
2. public: 客户端和代理服务器都可以缓存
3. max-age=xxx: 缓存的内容将在 xxx 秒后失效
4. no-cache: 强制向服务端再验证一次, 会将对应的缓存存储在客户端, 但是我们在下次用的时候, 要向服务端验证一次这个缓存到底是能用还是不能用再决定是否用这个缓存
5. no-store: 不缓存请求的任何返回内容, 每次都发起新的请求

![https://miaomiaoqi.github.io/images/project/seckill/seckill_9.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_9.png)

**有效性判断(强制验证, 协商逻辑)**

ETag: 资源唯一标识

If-None-Match: 客户端发送的匹配 ETag 标识符

Last-modified: 资源最后被修改的时间

If-Modified-Since: 客户端发送的匹配资源最后修改时间的标识符

![https://miaomiaoqi.github.io/images/project/seckill/seckill_10.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_10.png)

**浏览器三种刷新方式**

回车刷新或 a 链接: 看 cache-control 对应的 max-age 是否仍然有效, 有效则直接 from cache, 若 cache-control 中为 no-cache, 则进入缓存协商逻辑

F5 刷新或 command + r 刷新: 去掉 cache-control 中的 max-age 或直接设置 max-age 为 0, 然后进入缓存协商逻辑

ctrl + F5(windows) 或 command + shift + r(mac) 刷新: 去掉 cache-control 和协商头, 强制刷新

协商机制, 比较 Last-modified 和 ETag 到服务端, 若服务端判断没变化则 304 不返回数据, 否则 200 返回数据



**对于动态请求, 服务端一般会返回 no-cache 但是却不返回 ETag 和 Last-modified 或者不返回该头(max-age=0), 那么下次请求就不会走协商逻辑**



### CDN 自定义缓存策略

虽然源站可以告诉我们可以缓存的时间, 但是 CDN 可以自定义自己的缓存策略

* 可自定义目录过期时间
* 可自定义后缀名过期时间
* 可自定义对应权重
* 可通过界面或 api 强制 cdn 对应目录刷新(非保成功)

![https://miaomiaoqi.github.io/images/project/seckill/seckill_11.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_11.png)

### 静态资源部署策略

css, js, img 等元素使用带版本号部署, 例如 a.js?v=1.0 不便利, 且维护困难

css, js, img 等元素使用带摘要部署, 例如 a.js?v=45edw 存在先部署 html 还是现部署资源的覆盖问题, 有可能导致线上不可用

css, js, img 等元素使用摘要做文件名部署, 开入 45edw.js, 新老笨笨并存且可回滚, 资源部署完后再部署 html(推荐此种)



对应静态资源保持生命周期内不会变, max-age 可设置的很长, 无视失效更新周期

html 文件设置 no-cache 或较短 max-age, 以便于更新

html 文件仍然设置较长的 max-age, 依靠动态的获取版本号请求发送到后端, 异步下载最新的版本号的 html 后展示渲染在前端



动态请求也可以净化成 json 资源推送到 cdn 上

依靠异步请求获取后端节点对应资源状态做紧急下架处理

可通过跑批紧急推送 cdn 内容以使其下架等操作



### 全页面静态化

在服务端完成 html, css, 甚至 js 的 load 渲染成纯 html 文件后直接以静态资源的方式部署到 cdn 上, 部署 cdn 可以有 sdk 使用, 当有数据发生变化时向一个服务推送消息, 这个服务专门用来生成静态页面并且调用 sdk 想 cdn 服务部署静态文件



## 交易性能优化-缓存库存

### 交易性能瓶颈

我们在创建订单的时候, 至少会访问 6 次数据库, 对性能产生了极大的影响

![https://miaomiaoqi.github.io/images/project/seckill/seckill_12.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_12.png)

```java
@Override
@Transactional(rollbackFor = Exception.class)
public OrderModel createOrder(Integer userId, Integer itemId, Integer promoId, Integer amount) throws BusinessException {
    // 1. 校验下单状态,下单的商品是否存在，用户是否合法，购买数量是否正确
    ItemModel itemModel = this.itemService.getItemById(itemId);
    if (itemModel == null) {
        throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "商品信息不存在");
    }

    UserModel userModel = this.userService.getUserById(userId);
    if (userModel == null) {
        throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "用户信息不存在");
    }
    if (amount <= 0 || amount > 99) {
        throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "数量信息不正确");
    }

    // 校验活动信息
    if (promoId != null) {
        //（1）校验对应活动是否存在这个适用商品
        if (promoId.intValue() != itemModel.getPromoModel().getId()) {
            throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "活动信息不正确");
            //（2）校验活动是否正在进行中
        } else if (itemModel.getPromoModel().getStatus().intValue() != 2) {
            throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "活动信息还未开始");
        }
    }

    // 2. 落单减库存(还有一种支付减库存, 存在超卖现象)
    boolean result = this.itemService.decreaseStock(itemId, amount);
    if (!result) {
        throw new BusinessException(EmBusinessError.STOCK_NOT_ENOUGH);
    }

    // 3. 订单入库
    OrderModel orderModel = new OrderModel();
    orderModel.setUserId(userId);
    orderModel.setItemId(itemId);
    orderModel.setAmount(amount);
    if (promoId != null) {
        orderModel.setItemPrice(itemModel.getPromoModel().getPromoItemPrice());
    } else {
        orderModel.setItemPrice(itemModel.getPrice());
    }
    orderModel.setPromoId(promoId);
    orderModel.setOrderPrice(orderModel.getItemPrice().multiply(new BigDecimal(amount)));

    // 生成交易流水号,订单号
    orderModel.setId(this.generateOrderNo());
    OrderDO orderDO = this.convertFromOrderModel(orderModel);
    this.orderDOMapper.insertSelective(orderDO);

    // 加上商品的销量
    this.itemService.increaseSales(itemId, amount);
    // 4. 返回前端
    return orderModel;
}
```



### 交易验证优化

#### **用户风控策略优化**

策略缓存模型化, 将用户的验证信息放到 redis 中一份, 减少后续的查库操作

#### **活动校验策略优化**

引入活动发布流程, 模型缓存化, 紧急下线能力

#### **扣减库存优化**

1. 表锁改行锁

    我们在对库存操作时, 没有加索引, 导致会锁住整张表, 需要将 item_id 列加入索引将表锁改为行锁

    ```sql
    update item_stock
    set stock = stock - #{amount}
    where item_id = #{itemId} and stock >= #{amount}
    ```

    ```sql
    ALTER TABLE item_stock ADD UNIQUE INDEX item_id_index(item_id);
    ```

    针对同一个商品在数据库中的扣减库存操作的串行化是不可避免的, 所以我们要进一步优化将数据库操作改为内存操作

2. 扣减库存缓存化

    虽然在内存中扣减库存也是串行的, 但是在内存中的消耗几乎可以忽略不计, 此处要保证内存和数据库的库存一致性

    1. 活动发布同步库存进缓存(同时上架商品)
    2. 下单交易减缓存库存

3. 异步同步数据库

    在内存中扣减库存后, 因为内存中的数据不具备持久性, 需要同步到数据库, 我们采用异步的方式同步

    1. 异步消息扣减数据库库存表, 通过使用 rocketmq

        部署模型

        ![https://miaomiaoqi.github.io/images/project/seckill/seckill_13.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_13.png)

        ![https://miaomiaoqi.github.io/images/project/seckill/seckill_14.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_14.png)

    2. 分布式事务

        ![https://miaomiaoqi.github.io/images/project/seckill/seckill_15.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_15.png)

        我们的系统中, 在 redis 中扣减了内存, 还没有同步到数据库, 这个时候就是**软状态**, 是可以容忍的, 但要保证最终数据库和内存中的数据一致

    3. 保证库存数据库最终一致性

        1. 下载安装 rocketmq

            ```bash
            unzip rocketmq-all-4.6.0-bin-release.zip
            cd rocketmq-all-4.6.0-bin-release
            ```

        2. 启动 name server

            ```bash
            nohup sh bin/mqnamesrv &
            tail -f ~/logs/rocketmqlogs/namesrv.log
            
            lsof -i :9876 查看是否启动 name server
            ```

        3. 启动 broker

            ```bash
            nohup sh bin/mqbroker -n localhost:9876 &
            tail -f ~/logs/rocketmqlogs/broker.log
            ```

        4. 发送/接收消息

            ```bash
             > export NAMESRV_ADDR=localhost:9876
             > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer
             SendResult [sendStatus=SEND_OK, msgId= ...
            
             > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer
             ConsumeMessageThread_%d Receive New Messages: [MessageExt...
            ```

        5. 停止服务

            ```bash
            > sh bin/mqshutdown broker
            The mqbroker(36695) is running...
            Send shutdown request to mqbroker(36695) OK
            
            > sh bin/mqshutdown namesrv
            The mqnamesrv(36664) is running...
            Send shutdown request to mqnamesrv(36664) OK
            ```

        6. 创建 topic

            ```bash
            bin/mqadmin updateTopic -n localhost:9876 -t stock -c DefaultCluster
            ```

        7. 修改下单逻辑

#### 目前存在的问题

异步消息发送失败(已解决), catch 异常进行内存回滚

数据库扣减操作执行失败(因为是异步的, 此时订单已经生成了)

下单失败无法正确回补库存(此时已下单, 但是用户迟迟没有支付, 此时无法自动回滚库存)



## 交易性能优化-事务型消息