---
layout: post
title: 高性能秒杀
categories: [project]
description: 
keywords: 
---

* content
{:toc}


## 云端部署

### 数据库部署

备份

```
mysqldump -uroot -p --databases seckill > ~/Downloads/seckill.sql
```

导入

```
mysql -uroot -p < /usr/local/seckill.sql
```

### 打包上传

本地打包

```bash
mvn clean package
```

上传到服务器

```bash
scp imooc-seckill-1.0-SNAPSHOT.jar root@122.51.237.121:/usr/local
```

登录服务器创建目录

```bash
mkdir -p /var/www/seckill
```

移动 jar 包

```bash
cd /var/www/seckill
mv /usr/local/imooc-seckill-1.0-SNAPSHOT.jar seckill.jar
```

授权

```bash
chmod -R 777 *
```

启动程序

```bash
java -jar seckill.jar
```



### 编写 deploy

```bash
cd /var/www/seckill
```

编写外挂配置

```bash
vim application.properties

server.port=8088
```

重新启动程序, 指定外挂配置

```bash
java -jar seckill.jar --spring.config.addition-locaotion=/var/www/seckill/application.properties
```

编写 deploy 脚本

```bash
vim deploy.sh

nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar seckill.jar --spring.config.addition-locaotion=/var/www/seckill/application.properties
```



## jemeter 压测

### 线程组

### http 请求

### 查看结果树

### 聚合报告

![https://miaomiaoqi.github.io/images/project/seckill/seckill_4.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_4.png)

90% Line: 90% 的请求落在这个毫秒内, 反之就是有 10% 的请求大于这个毫秒

95% Line: 95% 的请求落在这个毫秒内

99% Line: 95% 的请求落在这个毫秒内

### 查看服务器状态

使用 top 命令可以查看 cpu 的使用状态

```bash
top -H
```

![https://miaomiaoqi.github.io/images/project/seckill/seckill_5.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_5.png)

**CPU usage: 7.12% user, 6.3% sys:** 分表代表用户态和系统态的 CPU 使用率, 加在一起不超过 100%

**Load Avg: 2.88, 3.10, 3.08:** 1 分钟, 5 分钟, 15 分钟 CPU 的使用负载, 不超过服务器 CPU 个数最好, load 越高说明 CPU 耗的越多

## Tomcat 调优

我们使用 5000 个线程循环 20 次对接口 /item/get?id=8 进行压测, 最终会报错显示 tomcat 拒绝连接, 初步判断是 tomcat 的线程数不够导致的该问题

### 查看线程数

通过使用 `pstree -p pid | wc -l` 查看 java 服务的线程数发现只有 39 个线程(29 个其他线程), 导致客户端请求不能建立新的连接发生报错 mac 使用`ps -M pid | wc -l`

### **查看内嵌 tomcat 配置**

spring-boot-autoconfigure 包下的 spring-configuration-metadata.json 文件查看各个节点的配置可以看到内嵌 tomcat 的默认配置

**默认内嵌 tomcat 配置**

server.tomact.accept-count: 等待队列长度, 默认 100

server.tomcat.max-connections: 最大可被连接数, 默认 10000

server.tomcat.max-threads: 最大工作线程数, 默认 200

server.tomcat.min-spare-threads: 最小工作线程数, 默认 10

默认配置下, 连接超过 10000 后出现拒绝连接情况

默认配置下, 触发的请求超过 200+100 后拒绝处理

**理解 maxConnections, maxThreads, acceptCount 关系**

我们可以把tomcat比做一个火锅店，流程是取号、入座、叫服务员，可以做一下三个形象的类比：

1. acceptCount 最大等待数

    可以类比为火锅店的排号处能够容纳排号的最大数量；排号的数量不是无限制的，火锅店的排号到了一定数据量之后，服务往往会说：已经客满。

2. maxConnections 最大连接数

    可以类比为火锅店的大堂的餐桌数量，也就是可以就餐的桌数。如果所有的桌子都已经坐满，则表示餐厅已满，已经达到了服务的数量上线，不能再有顾客进入餐厅了。

3. maxThreads 最大线程数

    可以类比为厨师的个数。每一个厨师，在同一时刻，只能给一张餐桌炒菜，就像极了JVM中的一条线程

**整个就餐的流程, 大致如下**

1. 取号: 如果maxConnections连接数没有满，就不需要取号，因为还有空余的餐桌，直接被大堂服务员领上餐桌，点菜就餐即可。如果 maxConnections 连接数满了，但是取号人数没有达到 acceptCount，则取号成功。如果取号人数已达到acceptCount，则拿号失败，会得到Tomcat的Connection refused connect 的回复信息。
2. 上桌: 如果有餐桌空出来了，表示maxConnections连接数没有满，排队的人，可以进入大堂上桌就餐。
3. 就餐: 就餐需要厨师炒菜。厨师的数量，比顾客的数量，肯定会少一些。一个厨师一定需要给多张餐桌炒菜，如果就餐的人越多，厨师也会忙不过来。这时候就可以增加厨师，一增加到上限maxThreads的值，如果还是不够，只能是拖慢每一张餐桌的上菜速度，这种情况，就是大家常见的“上一道菜吃光了，下一道菜还没有上”尴尬场景。

### 修改配置

```properties
server.port=8088
# 等待队列长度, 默认100
server.tomact.accept-count=1000
# 最大工作线程数, 默认200, 4核8g内存, 线程数经验值800
# 操作系统做线程之间的切换调度是有系统开销的, 所以不是越多越好
server.tomcat.max-threads=800
# 最小工作空闲线程数, 默认10, 适当增大一些, 以便应对突然增长的访问量
server.tomcat.min-spare-threads=100
```

**修改参数后启动程序使用 ps -M pid|wc -l 查看发现线程数增多到 129 个, 再次使用 2000 个线程循环 50 次进行压测, 可以看到线程数增加到 832 个, 虽然最终也会报错, 但是不会像默认配置一样很快就发生报错, 说明提高 tomcat 线程数可以增强系统的访问能力**



### 定制化内嵌 Tomcat 开发

**如果我们 keepalive 一直保持连接, 但是网页一直没有操作, 就会占用线程浪费资源**

keepAliveTimeOut: 多少毫秒后不响应的断开 keepalive

maxKeepAliveRequests: 多少次请求后 keepalive 断开失效

使用 WebServerFactoryCustomizer\<ConfigurableServletWebServerFactory\> 定制化内嵌 tomcat 配置

```java
/**
 * 当 Spring 容器内没有 TomcatEmbeddedServletContainerFactory 这个 bean 时，会吧此 bean 加载进 spring 容器中
 *
 * @author miaoqi
 * @date 2020-01-29
 *
 * @return
 */
@Component
public class WebServerConfiguration implements WebServerFactoryCustomizer<ConfigurableWebServerFactory> {

    @Override
    public void customize(ConfigurableWebServerFactory factory) {
        // 使用对应工厂类提供给我们的接口定制化我们的 tomcat connector
        ((TomcatServletWebServerFactory) factory).addConnectorCustomizers(new TomcatConnectorCustomizer() {
            @Override
            public void customize(Connector connector) {
                Http11NioProtocol protocol = (Http11NioProtocol) connector.getProtocolHandler();
                // 定制化keepalivetimeout,设置30秒内没有请求则服务端自动断开keepalive链接
                protocol.setKeepAliveTimeout(30000);
                // 当客户端发送超过10000个请求则自动断开keepalive链接
                protocol.setMaxKeepAliveRequests(10000);
            }
        });
    }

}
```



### 单 Web 容器上线

**线程数量:** 4 核 cpu 8G 内存单进程调度线程数 800-1000以上后即花费巨大的时间在 cpu 调度上

**等待队列长度:** 队列做缓冲池用, 但也不能无限长, 消耗内存, 出队入队也消耗 cpu



### MySql 数据库 QPS 容量问题

**主键查询:** 千万级别数据 = 1-10 毫秒

**唯一索引查询:** 千万级别数据 = 10-100 毫秒

**非唯一索引查询:** 千万级别数据 = 100-1000 毫秒

**无索引:** 百万条数据 = 1000 毫秒+

### MySql 数据库 TPS 容量问题

更新删除操作: 同查询

插入操作: 1W ~ 10W tps(依赖配置优化)





## 分布式扩展

![https://miaomiaoqi.github.io/images/project/seckill/seckill_6.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_6.png)

我使用 600 个线程, 循环 50 次, 10 秒内启动完成, 理论上每秒应该处理 3000 个请求, 而实际上 tps 只有 2500 左右, **证明单机达到了上限, 多余的请求进行排队处理**

**单机容量问题, 水平扩展**

表象: 单机 cpu 使用率增高, memory 占用增加, 网络带宽使用增加

cpu us: 用户空间的 cpu 使用情况(用户层代码)

cpu sy: 内核空间的 cpu 使用情况(系统调用)

load average: 1,5,15 分钟 load 平均值, 根据核数系数, 0 代表通常, 1(核数) 代表打满, 1+代表等待阻塞

memory: free 空闲内存, used 使用内存

### nginx 反向代理负载均衡

到 [http://openresty.org/en](http://openresty.org/en) 下载 openresty(基于 nginx 做了二次封装许多 lua 脚本, 方便使用)

使用 `sbin/nginx -c /usr/local/etc/openresty/nginx.conf` 命令启动 openresty

location 节点 path: 指定 url 映射 key

location 节点内容: root 指定 location path 后对应的根路径, index 指定默认的访问页

sbin/nginx -c conf/nginx.conf 启动

修改配置后直接 sbin/nginx -s reload 无缝重启

#### nginx 部署前端资源

1. 拷贝前端资源到 openresty 的 html/resources 目录下

2. 修改 nginx.conf 配置文件, 将所有 resources/ 请求都映射到 html/resources 目录下

    ```yaml
    server {
      listen       80;
      server_name  localhost;
    
      #charset koi8-r;
    
      #access_log  logs/host.access.log  main;
    
      #location / {
      	#root   html;
      	#index  index.html index.htm;
      #}
      #
      location /resources/ {
      	alias /usr/local/Cellar/openresty/1.15.8.2/nginx/html/resources/;
      	index index.html index.htm;
      }
    
      #error_page  404              /404.html;
    
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      	location = /50x.html {
      	root   html;
      }
    
      # proxy the PHP scripts to Apache listening on 127.0.0.1:80
      #
      #location ~ \.php$ {
      #    proxy_pass   http://127.0.0.1;
      #}
    
      # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
      #
      #location ~ \.php$ {
      #    root           html;
      #    fastcgi_pass   127.0.0.1:9000;
      #    fastcgi_index  index.php;
      #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
      #    include        fastcgi_params;
      #}
    
      # deny access to .htaccess files, if Apache's document root
      # concurs with nginx's one
      #
      #location ~ /\.ht {
      #    deny  all;
      #}
    }
    ```

#### nginx 反向代理服务端

1. 设置 upstream server 指明后端服务地址

    ```yaml
    upstream backend_server {
    	server 127.0.0.1:8088 weight=1;
    	server 127.0.0.1:8089 weight=1;
    	# 开启 nginx 与 server 的 keepalive, 设置为 30 秒
    	keepalive 30;
    }
    ```

2. 设置动态请求 location 为 proxy pass 路径

    ```yaml
    location / {
    	# 将除去 /resource/ 的请求都当做动态请求, 进行反向代理, 轮询请求后端服务
      proxy_pass http://backend_server;
      # 经过代理 nginx 就成为了 client 端, 需要将原始 Host 传到服务端
      proxy_set_header Host $http_host:$proxy_port;
      # 传入真正的访问 ip
      proxy_set_header X-Real-IP $remote_addr;
      # nginx 作为代理服务器转发了请求
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      
      # 默认使用 http1.0 与 Connection=closed, 即用完就断开连接, 不支持 keepalive
      # 需要手动指定 http1.1 与 Connection="", 即代表支持 keepalive
      proxy_http_version 1.1;
      proxy_set_header Connection "";
    }
    ```

3. 开启 tomcat access log 验证, 修改 springboot 配置文件, 线上建议开启

    ```
    server.tomcat.accesslog.enabled=true
    server.tomcat.accesslog.directory=/Users/miaoqi/Documents/seckill/seckill8088/tomcat
    server.tomcat.accesslog.pattern=%h %l %u %t "%r" %s %b %D
    ```

    %h: 远端 host 即 ip 地址

    %l: 

    %u: 远端 user

    %t: 处理时长

    %r: http 请求的第一行

    %s: 返回状态码

    %b: 请求 response 的大小

    %D: 处理请求的时长

#### nginx 高性能的原因

1. epoll 多路复用

    java bio 模型, 阻塞进程式

    linux select 模型, 变更触发轮询查找, 有 1024 数量上限

    epoll 模型, 变更触发回调直接读取, 理论上无上限

2. master worker 进程模型

    ![https://miaomiaoqi.github.io/images/project/seckill/seckill_7.png](https://miaomiaoqi.github.io/images/project/seckill/seckill_7.png)

    nginx 启动必定会启动一个 master 进程, 随后会根据 nginx.conf 中 worker_processes 的配置数量 fork 出相应数量的 worker 进程

    master 进程监听 80 端口, 当有一个连接请求过来时, 会让 worker 进程在内存中抢锁的方式来决定哪个 worker 进程接管这个 socket, worker 进程执行 accept 操作建立 socket 并且设置回调, 后续所有的操作都在该 worker 中完成

    `nginx -s reload` 操作不会重启 master 进程且已经连接的 socket 不受影响, master 会接管 worker 的 socket, 并且重新启动一个新的 worker 加载配置文件, 再将 socket 交给 worker 进程, 整个过程是在内存中完成的, 效率非常高

3. 协程机制

    依附于线程的内存模型, 切换开销小

    遇阻塞及归还执行权, 代码同步

    无需加锁



### 分布式会话管理

### 使用 redis 实现分布式会话存储